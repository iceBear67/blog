{"pages":[{"title":"Lnks","text":"本博客名称： 非标准输出 HikariLan’s Blog贺兰星辰的博客 一实无言伏太的博客 一叶知秋 - 秋是悲欢离合，雨是一生错过 秋雨落的博客 LxNetLxns的博客LxnsNB czm的博客 - 记想记的东西 EvanLuo42’s Blog","link":"links/index.html"},{"title":"关于 nstd::out","text":"Non-STandard Output 是 iceBear 的破事水博客。你可以在这里了解到 iceBear","link":"about/index.html"},{"title":"空调房","text":"我们在此处安装了空调，您可以在此自由休憩。","link":"%E7%A9%BA%E8%B0%83%E6%88%BF/index.html"}],"posts":[{"title":"Hello World","text":"你好, Hexo！折腾了一个晚上，我终于还是回到 Hexo 的怀抱了。接下来会陆陆续续更新一些杂文，并且把我在 Lxnet 发布的那几篇文章搬一下，欢迎 Watch/订阅 我的博客。","link":"2021/03/12/hello-world/"},{"title":"使用 cproxy 对程序进行透明代理","text":"在 Windows 下，说到 透明代理 ，你可能会想到 Proxifier , SSTap, Mellow但在 Linux 下，你可能会想到 proxychains 然后再想到 proxychains 不能代理 static-linked 的程序。 为什么 ProxyChains(NG) 不能代理静态连接的程序ProxyChains NG 通过一个预加载的共享库来hook到网络相关的函数(比如: connect,getaddrinfo)上，接着他就可以转发这些流量。缺点似乎显而易见，如果只是通过 LD_PRELOAD 来进行透明代理，那么这招对于静态连接的程序是无用的（比如说 Go 程序..)其次，ProxyChains 只支持 TCP，这可能导致无法转发 DNS 流量 -&gt; DNS 流量泄漏。 cproxy 是什么cproxy 也是透明代理，作用和 proxychains 类似。原理上与 proxychains 不同，它使用 cgroup 进行对程序的代理 ，不过配置起来还有点麻烦本文将会教您在您的Linux PC上使用 cproxy，截至本文发布之前，cproxy似乎只支持 Linux。 下载 cproxy官方推荐的方法是: 直接用 cargo 1cargo install cproxy 如果没有 cargo 可以在 Release 上下载压缩包。解压压缩包，可能会发现有两个可执行文件，不过似乎只留一个就好了。 安装 cproxy先来试一试 cproxy！ 1cproxy --port &lt;本地代理端口&gt; -- curl ip.sb ( ｀д′) 失败了如果你和我一样，在port上写了 socks5 的代理端口，那么你就被坑了。笔者填入 geph 的 HTTP 代理端口时是可以成功的，但是使用 v2ray 的s5/http端口都无法使用。这是因为 cproxy 需要修改 v2ray 的配置进行兼容，如果你不想去改配置 在确保可以使用 cproxy 后，我们来配置 alias。将以下内容写到 ~/.bashrc，记得自己修改一些部分。 alias &quot;px&quot;=&quot;/path/to/cproxy --port &lt;your-local-proxy-port&gt; --&quot; 例如我的配置： alias &quot;px&quot;=&quot;/bin/cproxy --port 60080 --use-tproxy -- &quot; # 60080 是ipt2socks的默认监听端口。 保存后，使用 source ~/.bashrc 载入，试试 px curl ip.sb，看看是否成功。 缺陷每次使用都要 sudo ，还是挺烦人的吧…对于这种情况，也可以考虑使用有后台常驻的 cgproxy，但本文不做介绍，有兴趣可以自己了解。","link":"2021/03/13/%E4%BD%BF%E7%94%A8-cproxy-%E5%AF%B9%E7%A8%8B%E5%BA%8F%E8%BF%9B%E8%A1%8C%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86/"},{"title":"修复 Intellij IDEA 无法使用中文输入法","text":"踩坑记录 TL;DR系统: ArchLinux一切的大前提: 环境变量设置正确(GTK_IM_MODULE…) ibus 用户可以试试迁移到 fcitx fcitx 如果不行可能得自己编译一个 patched 的 JetBrains Runtime 开始IDEA 没法输入中文的问题其实在我系统上盘踞很久了，一会能用一会不能的….所以我打算动手整顿他 设置环境变量学着 ArchLinux Wiki，我首先往 ~/.pam_environment 加了这些东西: 123GTK_IM_MODULE DEFAULT=ibusQT_IM_MODULE DEFAULT=ibusXMODIFIERS DEFAULT=\\@im=ibus 然而并没有什么效果… 加到 idea.sh 头上也不行，qtconfig-qt4 也改过了，没啥效果 修改键盘布局压根没变化 更换输入法于是我尝试更换到 fcitx （更换到 fcitx 之后似乎已经可以在 idea 窗口里面召唤出 fcitx 了，但是好像还是输入不了中文 自己编译在 archlinuxcn 论坛上找到了一个回复: 其实解决这个问题的过程还挺曲折的，我不打算直接给出顺利的解决方案，所以建议先看完，再动手操作，否则会踩老坑，以下是折腾记录：问题：fcitx搜狗输入法在idea环境中文输入不跟随光标解决方案参考链接： https://blog.csdn.net/u011166277/articl … /106287587辛酸史：先下载JetBrainsRuntime源码，https://github.com/JetBrains/JetBrainsRuntime因为github很慢，所以到https://gitee.com/上导入github进行下载，还有两种下载方式，教程都在https://zhuanlan.zhihu.com/p/121015450代下载网站https://shrill-pond-3e81.hunsh.workers.dev/ http://g.widyun.com/ 网速慢的话容易过期下载完JetBrainsRuntime之后下载idea.patch https://github.com/prehonor/myJetBrainsRuntime然后把idea.patch拷贝到JetBrainsRuntime根目录git checkout cfc3e87f2ac27a0b8c78c729c113aa52535feff6 （这一步经过测试，并不需要，用最新的就好）git apply idea.patch然后根据官方教程编译就行了，但是，别信官方的，经过测试docker打包出现未知错误，还不知道咋查，ubuntu版本低了还不行，一会儿404，一会儿缺jdk11，拖过来一个jdk11又说glibc版本不够，安装glibc直接整个系统挂了，所有命令都不能用了，所以还是开个虚拟机，直接上ubuntu20吧，然后按照官方的教程编译$ sudo apt-get install autoconf make build-essential libx11-dev libxext-dev libxrender-dev libxtst-dev libxt-dev libxrandr-dev libcups2-dev libfontconfig1-dev libasound2-dev$ cd JetBrainsRuntime$ sh ./configure –disable-warnings-as-errors$ make images你以为这就完了吗？中途会直接出现killed错误，所以，内存至少2G，swap单开一个3G的$ dd if=/dev/zero of=swapfile bs=1024 count=310241024$ sudo mkswap swapfile$ sudo swapon swapfile$ free -h然后照常make images，编译完了之后，会生成jdk，从虚拟机中拷贝出来sudo scp -o PasswordAuthentication=yes -r 用户名@虚拟机ip:/home/用户名/JetBrainsRuntime/build/linux-x86_64-normal-server-release/images/jdk /usr/lib/jvm/java-11.0.7-jetbrains如果你以为按照教程里export IDEA_JDK=/usr/lib/jvm/java-11.0.7-jetbrains 就可以启动，那就又错了，还会报错/usr/lib/jvm/java-11.0.7-jetbrains/bin/java: symbol lookup error: /usr/lib/jvm/java-11.0.7-jetbrains/lib/libnio.so: undefined symbol: initInetAddressIDs你以为只要ldd 看到libjvm.so =&gt; not found就应该把/usr/lib/jvm/java-11.0.7-jetbrains/lib/server/libjvm.so 加到环境变量就行了？不，亲自试验过了，没用，对比了一下正常启动的idea，发现libnet.so路径不对，正常要取jdk里的，于是又加入环境变量，这回启动两次居然直接crash了最后，经过一系列折腾，搞出一个最佳解决办法，修改启动文件$ sudo vim /opt/intellij-idea-ultimate-edition/bin/idea.sh在开头加上export LD_LIBRARY_PATH=/usr/lib/jvm/java-11.0.7-jetbrains/lib:$LD_LIBRARY_PATH &amp;&amp; export IDEA_JDK=/usr/lib/jvm/java-11.0.7-jetbrain这回终于能启动成功了，编译好的jdk最好保存一下，别再这么折腾了 实际操作 ( px 是透明代理 )： 1234567px git clone https://github.com/JetBrains/JetBrainsRuntimecd JetBrainsRuntimepx wget https://raw.githubusercontent.com/prehonor/myJetBrainsRuntime/master/idea.patchgit apply ./idea.patchsh ./configure --disable-warnings-as-errorsmake images #其实这一步内存足够就不会出问题，不一定需要开swap什么的 编译使用的是liberica-11-openjdksudo mv ./build/linux-&lt;tab&gt;/images/jdk /usr/lib/jvm/jetbrains-11-openjdk 然后打开 IDEA，安装 Choose Runtime 插件添加自定义jdk，使用你刚编译的jetbrains runtime启动即可。（笔者机器上无报错）此处提供版本 jb11_0_11-b1504.12 的构建修改版（x86_64）: 下载","link":"2021/07/18/Idea/"},{"title":"从零开始的编译器生涯","text":"近日一屑高二学生无聊动手写起了编译器….这是他的珍贵作战记录 0x01 理论基础我摊牌，我没有看任何编译原理相关的书籍，因此这篇文章并不能作为严格的参考资料，甚至很多地方可能是错误的。 编译器，编译器，就是把高级语言的代码编译成另一种形式（class，asm，二进制，IR），而他在编译成另一种形式之前大概需要过这么个流程: Lex -&gt; Parse -&gt; Compile 接下来逐步讲解这个过程。 Lexer就是分词器，输入用户提供的代码接着把他分成 tokens，也就是 tokenstream。你肯定看不懂上面那句话的意思，让我们来点实例：a.value 里的那个 ArrayList 就是一个 token stream，str 是被解析的代码。不难发现，语句被 Lexer 按顺序进行了分类以及数据的分割，如 a 被识别为了一个 Identify (标记)。 因此也可以归纳出来 Token 大致的代码长啥样： 1234567@AllArgsConstructor@Getterpublic class Token { private int line; private Type type; private String content;} ParserLexer 从源码中提取出 token stream 后将会交给 Parser 处理，它负责对 token stream 进行解析，生成一个 AST (Abstract Syntax Tree)，也就是 抽象语法树。 这张图直观的描述了这一过程，你可以看到它以树状的形式表现编程语言的语法结构，树上的每个节点都表示源代码中的一种结构。之所以说语法是“抽象”的，是因为这里的语法并不会表示出真实语法中出现的每个细节。 接着，AST 将会丢给代码生成器用于生成代码，但是一般会先对 AST 进行优化，例如 常量折叠 Static Analyzing但在这之前，我们还有一些问题要解决。其实这玩意我是和 Parser 写一块的试想一下，如果有这样一行代码： 1var i = love + cats 代码生成器如何知道 love 和 cats 是什么？ 在 Parser 的眼里，他们只是 Identifier，然而它们之间不能相加减。 在这种时候，Parser 需要预先建立一个符号表，这样他才能找出 love 和 cats 究竟是什么以及是否能够编译。 同理，下面的代码也一样需要这一过程： 123456import java.util.Listvoid main(List&lt;String&gt; args){ // 此处 Parser 将会分析出 java.lang.String 和 java.util.List NullCat nc = new SBNC(); // 按照 Java 的逻辑，此处没有导入（或同包）于是会产生错误，因为Parser找不到 SBNC / NullCat} Code Generation接着是生成代码！一般编译器都会输出一种 IR (Intermediate Representation) 码，而他的作用则是一种中间表示。例如，如果你输出 LLVM 的 IR 码，那么接下来你的编译工作（win,x64,linux,…jvm）就可以交给 LLVM 来完成，而像 LLVM 这样负责最后这一步骤的我们称之为 编译器的后端 使用这一种方法有几个好处： 它可以使得开发者更专注于 语言设计 而不用过多的考虑 优化，因为大多数编译器后端会帮你完成这件事情 ，除非你直接输出汇编那就得你自己负责优化了。 IR 是中间表示，它可以按照相同的语义编译出不同平台，不同架构的代码，大大节省了开发者时间 … 处于个人习惯，我选择了 Java 的字节码作为 “IR”，他将会被 JVM 加载并在运行过程中收集数据被更好的优化以及可以享受和 Java 互操作，跨平台的优势。 0x02 实践知道了这些理论，我们立即可以开始编写我们的第一个 Lexer 了。 这是我们这一大章节的目标代码： 12345using java.util.Listfn main(args: List&lt;String&gt;){ println &quot;hello world!&quot;} 那么，让我们开始吧！下文将会有大量代码，为了可读性，我会删掉一些无关紧要的部分。 Lexer我的 Lexer 分为两步：fuzzyTokenize 和 tokenize。实际上这是取决于做法的，有正则转 DFA（状态机）的，也有直接 charStream 的。 我选择了第二种，因为我认为使用正则的代码可读性比较糟糕，不易于维护。那么，让我们开始做一些准备工作… 123456789public class Lexer { private final String fileName; private final String rawContent; public Lexer(String content,String fileName) { rawContent = content.replaceAll(&quot;//.*|(\\&quot;(?:\\\\\\\\[^\\&quot;]|\\\\\\\\\\&quot;|.)*?\\&quot;)|(?s)/\\\\*.*?\\\\*/&quot;, &quot;$1 &quot;); // remove comments. this.fileName=fileName; }} 从构造方法接受源代码和文件名并且删除注释。你可能会问文件名用来干啥，那当然是用来报错的～接着，还有一个 LexedNode 用来表示 fuzzyTokenize 后的产物。 1234567891011121314public class LexedNode { private NodeType type; private String content; // 初始化和getter... public enum NodeType { IDENTIFIER,SYMBOL,KEYWORD,OPERATOR, LINE_SEPERATOR, LITERAL_STRING,LITERAL_NUMBER }} 这就是一个最基本的 token! 在后文，我们将会进行第二次 tokenize 使它变得更详细。 准备好了，开始写吧！首先是一个状态机： 1234567891011121314public List&lt;LexedNode&gt; fuzzyTokenize() { char[] charStream = rawContent.toCharArray(); List&lt;LexedNode&gt; nodes = new ArrayList&lt;&gt;(); int line = 1; for (int i = 0; i &lt; charStream.length; i++) { // 使用 fori 是为了循环时移动指针 char now = charStream[i]; switch (now) { case '\\n': nodes.add(new LexedNode(NodeType.LINE_SEPERATOR,&quot;\\n&quot;)) continue; // 此处使用 continue 立即跳到下一次循环 } } return nodes;} 这就是你的第一个 Lexer，可以先输出一下看看结果： 123456789LINE_SEPERATORLINE_SEPERATORLINE_SEPERATORLINE_SEPERATORLINE_SEPERATOR 因为代码有五行，因此是五个 LINE_SEPERATOR。只有换行符可不够，我们还要识别 KEYWORD ，也就是关键词。然而关键词使用空格分割，因此我们可以这样做： 123456789101112131415161718192021switch(...){ //... case ' ': inIdOrLiteral = !inIdOrLiteral; if (inIdOrLiteral) { // start collecting continue; } // end! String str = buffer.toString(); identifierParse(str, nodes); buffer = new StringBuilder(); // compose continue; // 此处换行同理}//.../* Collect String or Identifier */ if (inIdOrLiteral) { buffer.append(now); continue; } 你可以看到，我们引入了两个新的变量和一个方法，它们分别是 inIdOrLiteral 和 buffer 以及 identifierParse。 inIdOrLiteral 表示当前是否正在遍历一个 Identifier 或者一个字面量buffer 用于收集这个字面量，当然你也可以使用 substring 和 charAt 的方法identifierParse 是一个方法，他用于分类 Identifier。对于 11，他会分类成一个 LITERAL_NUMBER，对于 not_a_keyword，他会分类成一个 identifier，对于 fn，他会分类成一个 Keyword。 还没完，天资聪颖的你肯定已经注意到了这里少了一样东西——我要怎么匹配最开头的一个 using ？ using 的前头可没有一个空格。这时你可以回忆一下，在各种编程语言中作为 Identifier 的符号应该符合什么规则….是的，他们通常不会以运算符作为开头，以及他们不是一个关键字，因此我们还可以利用这个特性写出这样的代码: 1234567891011121314151617181920212223242526 /* Other Symbols */ if (SYMBOL_OR_OPERATORS.contains(now）) { if (inIdOrLiteral) { // keyword // now == a symbol,we should end this. identifierParse(buffer.toString(), nodes); buffer = new StringBuilder(); inIdOrLiteral = false; } if (SYMBOLS.contains(now)) { nodes.add(new LexedNode(now, LexedNode.NodeType.SYMBOL)); continue; } else if (OPERATORS.contains(now)) { nodes.add(new LexedNode(now, LexedNode.NodeType.OPERATOR)); continue; } else { throw new LexerException(fileName+&quot;: Unknown char: &quot; + now+&quot; line: &quot;+line); } } else { inIdOrLiteral = true; // not symbol &amp; not identifier }/* Collect String or Identifier */ if (inIdOrLiteral) { buffer.append(now); continue; } 这一段代码将会在匹配第一个字符没有遇到语言规定的操作符或者特殊符号的时候把 inIdOrLiteral 设置为 true。配合上面的代码，在遇到一个空格的时候他会结束收集并且尝试判断是什么。 实际上应该是 switch 的任务但是写成 if 更加直观一些。 那么到现在，我们可以开始尝试代码了！这是 Lexer 的输出： 12345678910111213141516171819202122232425262728KEYWORD usingIDENTIFIER javaOPERATOR .IDENTIFIER utilOPERATOR .IDENTIFIER ListLINE_SEPERATOR LINE_SEPERATOR KEYWORD fnIDENTIFIER mainSYMBOL (IDENTIFIER argsOPERATOR :IDENTIFIER List&lt;String&gt;SYMBOL )SYMBOL {LINE_SEPERATOR KEYWORD println- LITERAL_STRING hello world!+ IDENTIFIER &quot;hello+ IDENTIFIER world!&quot;LINE_SEPERATOR SYMBOL }5: RIGHT_BRACKET } 相比你已经注意到了，理应出现的 LITERAL_STRING 被两个 IDENTIFIER 代替了，这显然不是我们想要的结果。因此，我们要给 String 加入特 殊 支 持 1234567891011121314151617switch(now){ case '&quot;': if (i != 0 &amp;&amp; charStream[i - 1] != '\\\\') { // string starts or end inIdOrLiteral = !inIdOrLiteral; stringMode = true; if (!inIdOrLiteral) { stringMode = false; // a new string! nodes.add(new LexedNode(buffer.toString(), LexedNode.NodeType.LITERAL_STRING)); buffer = new StringBuilder(); continue; } } continue; //...} 以及 1234 case ' ':+ if (stringMode) {+ break;+ } 12- if (SYMBOL_OR_OPERATORS.contains(now)) {+ if (SYMBOL_OR_OPERATORS.contains(now) &amp;&amp; !stringMode) { 这样我们就躲开了这个陷阱，完成了对于 String 的支持后，我们的 fuzzyTokenize 就做好了！ 关于 OPERATORS 和 SYMBOLS一门语言里的符号很多，你绝对不会想把他们一个个 add 到 list 里面的，但你可以写一个 loader 来解决这个问题 然后，是 tokenizer。fuzzyTokenize 输出的结果显然不足以交给 Parser 做解析，我们需要使i结果更加详细。 好在经过 fuzzyTokenize 后代码已经被格式化成了比较模糊的 Token Stream，这一点使我们写第二次 tokenize 的时候会轻松很多，因为你不会再见到 inIdOrLiteral 和 stringMode 这种让人抓狂的东西了。 首先，让我们从一个新的 Token 开始（你不会想和 LexedNode 混一块的）: 1234567891011121314151617181920212223242526272829@AllArgsConstructor@Getterpublic class Token { private int line; private Type type; private String content; public enum Type{ IDENTIFIER(&quot;&quot;), CLASS(&quot;class&quot;),FUNCTION(&quot;fn&quot;),ANNOTATION(&quot;annotation&quot;),FOR(&quot;for&quot;),WHILE(&quot;while&quot;),IF(&quot;if&quot;),USING(&quot;using&quot;) ,THIS(&quot;this&quot;),TRUE(&quot;true&quot;),FALSE(&quot;false&quot;),ELSE(&quot;else&quot;),VAR(&quot;var&quot;),NULL(&quot;null&quot;),PRINTLN(&quot;println&quot;), // KEYWORDS VAL(&quot;val&quot;), LEFT_BRACE(&quot;(&quot;),RIGHT_BRACE(&quot;)&quot;), LEFT_BRACKET(&quot;{&quot;),RIGHT_BRACKET(&quot;}&quot;), LEFT_MID_BRACE(&quot;[&quot;),RIGHT_MID_BRACE(&quot;]&quot;), COMMA(&quot;,&quot;),DOT(&quot;.&quot;),MINUS(&quot;-&quot;),PLUS(&quot;+&quot;),STAR(&quot;*&quot;),SLASH(&quot;/&quot;), // operators BREAK_LINE(&quot;\\n&quot;),ASSIGNMENT(&quot;=&quot;),EQUALS(&quot;==&quot;),SEMICOLON(&quot;;&quot;),AT(&quot;@&quot;),COLON(&quot;:&quot;), LITERAL_STRING(&quot;&quot;),LITERAL_NUMBER(&quot;&quot;); // literals @Getter private String def; Type(String def){ this.def=def; } }} 比上文的 LexedNode 详细了很多——比如他主动去分类 keyword 了。接着是一个 fori ： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576public Pair&lt;String,List&lt;Token&gt;&gt; tokenize() { var lexedNodes = fuzzyTokenize(); var tokens = new ArrayList&lt;Token&gt;(); var line = 1; for (int i = 0; i &lt; lexedNodes.size(); i++) { LexedNode lexedNode = lexedNodes.get(i); switch (lexedNode.getType()) { case LINE_SEPERATOR: tokens.add(new Token(line, Token.Type.BREAK_LINE,&quot;&quot;)); line++; break; case SYMBOL: case KEYWORD: var type = Arrays.stream(Token.Type.values()).filter(e -&gt; e.getDef().equals(lexedNode.getContent())).findFirst().orElseThrow(()-&gt;{ return new NullPointerException(lexedNode.toString()); }); tokens.add(new Token(line, type, type.getDef())); break; case LITERAL_STRING: tokens.add(new Token(line, Token.Type.LITERAL_STRING, lexedNode.getContent())); break; case LITERAL_NUMBER: tokens.add(new Token(line, Token.Type.LITERAL_NUMBER, lexedNode.getContent())); break; case OPERATOR: // = boolean isEnd = (i == lexedNodes.size() - 1); switch (lexedNode.getContent()) { case &quot;=&quot;: if (isEnd) { throw new LexerException(fileName+&quot;: Invalid syntax line &quot;+line); } if (lexedNodes.get(i + 1).getType() == LexedNode.NodeType.OPERATOR &amp;&amp; lexedNodes.get(i + 1).getContent().equals(&quot;=&quot;)) { // == tokens.add(new Token(line, Token.Type.EQUALS, &quot;==&quot;)); i = i + 1; // skip next break; } else { tokens.add(new Token(line, Token.Type.ASSIGNMENT, &quot;=&quot;)); } break; case &quot;.&quot;: tokens.add(new Token(line, Token.Type.DOT, &quot;.&quot;)); break; case &quot;,&quot;: tokens.add(new Token(line, Token.Type.COMMA, &quot;,&quot;)); break; case &quot;-&quot;: tokens.add(new Token(line, Token.Type.MINUS, &quot;-&quot;)); break; case &quot;+&quot;: tokens.add(new Token(line, Token.Type.PLUS, &quot;+&quot;)); break; case &quot;*&quot;: tokens.add(new Token(line, Token.Type.STAR,&quot;*&quot;)); break; case &quot;/&quot;: tokens.add(new Token(line, Token.Type.SLASH,&quot;/&quot;)); break; case &quot;;&quot;: tokens.add(new Token(line, Token.Type.SEMICOLON,&quot;;&quot;)); break; case &quot;:&quot;: tokens.add(new Token(line,Token.Type.COLON,&quot;:&quot;)); break; } break; case IDENTIFIER: tokens.add(new Token(line, Token.Type.IDENTIFIER, lexedNode.getContent())); break; } } return Pair.of(fileName,tokens);} 这段代码并不难懂。在这个例子中，我们遍历来自 fuzzyTokenizer 的数据并且通过 switch 分类枚举来处理把他们转化成 Token 来表达并且存储到 tokens。对于 symbol 和 keyword，我们通过直接搜索 enum 内值的方法避免写出了像 case OPERATOR 里更糟糕的代码。 case OPERATOR 里写成这样是为了双符号操作的支持，例如 == 回到原题，这次我们可以通过 tokenize 解析出这样的结果： 123456789101112131415161718192021221: USING using1: IDENTIFIER java1: DOT .1: IDENTIFIER util1: DOT .1: IDENTIFIER List1: BREAK_LINE 2: BREAK_LINE 3: FUNCTION fn3: IDENTIFIER main3: LEFT_BRACE (3: IDENTIFIER args3: COLON :3: IDENTIFIER List&lt;String&gt;3: RIGHT_BRACE )3: LEFT_BRACKET {3: BREAK_LINE 4: PRINTLN println4: LITERAL_STRING hello world!4: BREAK_LINE 5: RIGHT_BRACKET }5: RIGHT_BRACKET } 是不是详细了很多？接着我们就可以靠着这个写一个 Parser了 在 Parse 之前在 Parse 之前，我们需要先做一次 Static Analyzing。在这个阶段，Parser 会对文件里的类型和导入表作出关联，同时也是多文件编译的基础。 Metadata你不可能靠着所有人的源码来建立索引，而且源码中的无用信息太多了。实际上，确定符号链接只需要这些信息： 1234567@Datapublic class CatMetadata { private ClassDef classDefinition = new ClassDef(); private Map&lt;String,CatMetadata&gt; cachedUsings = new HashMap&lt;&gt;(); // 这是对于被解析对象才有的 private List&lt; MethodSign&gt; methods = new ArrayList&lt;&gt;(); private Map&lt;String, VariableDef&gt; fields = new HashMap&lt;&gt;();} 关于 ClassDef, MethodSign, VariableDef 等信息本文不贴出，因为并不会影响观看体验。如果有兴趣，可以在这里找到他们相对应的具体代码 以及一个编译器全局索引，用 FQDN 确定唯一性的 Map: 123456789101112131415public static class Global { private static final Map&lt;String,CatMetadata&gt; GLOBAL_METADATAS = new HashMap&lt;&gt;(); public static final CatMetadata forClass(String str){ /* * Scan compiler classPaths */ var meta = NullCatCompiler.solveMeta(str); if(meta!=null) { GLOBAL_METADATAS.put(str, meta); }else{ meta = NullCatCompiler.solveMeta(&quot;java.lang.&quot; + str); } return meta; } } 准备就绪，我们来单独拿出一个类作为 MetadataGenerator 状态机 1234567@RequiredArgsConstructorpublic class MetadataGenerator { private final String fileName; private final List&lt;Token&gt; tokens; private CatMetadata cm = new CatMetadata(); private int i=0;} 接着，是提取数据的部分: 1234567891011121314151617181920212223242526272829303132333435363738public CatMetadata gen(){ for (i = 0; i &lt; tokens.size(); i++) { Token now = tokens.get(i); boolean end = (i==tokens.size()-1); Token next = end?null:tokens.get(i+1); switch(now.getType()){ case USING: if(!end){ i=i+1; var clazz = readAsStringUntilLB(); cm.getCachedUsings().put(clazz, Optional.ofNullable(CatMetadata.Global.forClass(clazz)).orElseThrow(()-&gt;new ParseException(&quot;Can't find clazz &quot;+clazz))); }else{ throwEOF(); } continue; case FUNCTION: if (end) { throwEOF(); } // fn main(){} if(next.getType() != Token.Type.IDENTIFIER){ throw new ParseException(fileName+&quot;: Unexcepted &quot;+next.getType()+&quot; at line &quot;+now.getLine()); } String methodName = next.getContent(); i=i+1; // Move Pointer to ( MethodSign sign = readMethodSign(methodName); if(cm.getMethods().stream().anyMatch(e-&gt;e.hashCode()==sign.hashCode())){ throw new ParseException(fileName+&quot;: Duplicated method: &quot;+sign+&quot; at line &quot;+now.getLine()); } cm.getMethods().add(sign); skipCodeBlocks(); continue; // ... } } return cm;} 在这个循环当中，我们通过获取到 Token 的类型来判定需要做的操作，这是基于语言设计定义来做的—— 例如 fn 的后面必然是一个方法签名，而不可以是别的。最终 MetadataGenerator 将会返回一个 CatMetadata 以供后续操作。 因此，这一阶段我们也可以发掘出类型错误和大的语法错误。 与 Java 的世界我们需要和 Java 交互，因此我们需要给 Class 建立 CatMetadata 。好在这很简单，因为 CatMetadata 需要的所有数据都可以通过反射获取，这里提供一段参考代码： 123456789101112131415161718192021222324252627282930@AllArgsConstructorpublic class ClassMetaPathImpl implements MetaPath{ private ClassLoader classLoader; @Override public CatMetadata findClass(String clazz) { CatMetadata cm = new CatMetadata(); Class&lt;?&gt; claz = Util.runCatching(()-&gt;{ return Class.forName(clazz,false,classLoader); }).getResult(); if(claz==null){ return null; } for (Field declaredField : claz.getDeclaredFields()) { if(!Modifier.isPublic(declaredField.getModifiers())) continue; VariableDef def = new VariableDef(declaredField.getType().getCanonicalName(),declaredField.getName()); cm.getFields().put(declaredField.getName(),def); } for(Method declaredMethod: claz.getDeclaredMethods()){ if(!Modifier.isPublic(declaredMethod.getModifiers()))continue; MethodSign sign = new MethodSign(declaredMethod.getName(), (ArrayList&lt;String&gt;) Arrays.stream(declaredMethod.getParameterTypes()).map(e-&gt;e.getCanonicalName()).collect(Collectors.toList())); cm.getMethods().add(sign); } ClassDef cdf = new ClassDef(); cdf.setClassName(clazz); cdf.setSuperclass(claz.getSuperclass()==null?null:claz.getSuperclass().getCanonicalName()); cdf.setInterfaces(Arrays.stream(claz.getInterfaces()).map(e-&gt;e.getCanonicalName()).collect(Collectors.toList())); cm.setClassDefinition(cdf); return cm; }} 静态分析结束后，我们就要准备开始生成 AST 了。 附我们从 token 流中获取数据，并且根据类型进行匹配——但我们其实没有用到状态仔细看，你会发现这个东西： 123456789 i=i+1; // Move Pointer to (+ MethodSign sign = readMethodSign(methodName); if(cm.getMethods().stream().anyMatch(e-&gt;e.hashCode()==sign.hashCode())){ throw new ParseException(fileName+&quot;: Duplicated method: &quot;+sign+&quot; at line &quot;+now.getLine()); } cm.getMethods().add(sign);+ skipCodeBlocks(); continue; 是不是有些象是 DSL？这其实归咎于类字段中那个不起眼的 int i = 0，它使得 for 循环的指针可以被整个类里的方法所共享。 12345678910private final String readAsStringUntilLB(){ // 一只读，读到一个换行为止并且收集成字符串 StringBuilder sb = new StringBuilder(); int b=0; for(int a = i; tokens.get(a).getType()!= Token.Type.BREAK_LINE;a++){ sb.append(tokens.get(a).getContent()); b=a; } i = b; return sb.toString();} 在经过更加详细的 tokenize 之后，代码实际上变得更加可观了， Parser先占个坑位～","link":"2021/10/01/Writting-A-Compiler-1/"}],"tags":[{"name":"tech,linux","slug":"tech-linux","link":"tags/tech-linux/"},{"name":"idea,fcitx,ibus","slug":"idea-fcitx-ibus","link":"tags/idea-fcitx-ibus/"},{"name":"tech","slug":"tech","link":"tags/tech/"}],"categories":[]}